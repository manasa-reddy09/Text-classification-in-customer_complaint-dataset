{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f348cb43-73fe-4332-ad78-95b3a8620730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a270fe6f-5c9d-4dd8-ad63-27f427f67ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"complaints.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22438322-c839-435b-b0fe-173183d2fcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Date received                                            Product  \\\n",
      "0    2020-07-06  Credit reporting, credit repair services, or o...   \n",
      "1    2025-09-24  Credit reporting or other personal consumer re...   \n",
      "2    2019-12-26                        Credit card or prepaid card   \n",
      "3    2020-05-08  Credit reporting, credit repair services, or o...   \n",
      "4    2025-09-23  Credit reporting or other personal consumer re...   \n",
      "\n",
      "                                  Sub-product  \\\n",
      "0                            Credit reporting   \n",
      "1                            Credit reporting   \n",
      "2  General-purpose credit card or charge card   \n",
      "3                            Credit reporting   \n",
      "4                            Credit reporting   \n",
      "\n",
      "                                               Issue  \\\n",
      "0               Incorrect information on your report   \n",
      "1               Incorrect information on your report   \n",
      "2  Advertising and marketing, including promotion...   \n",
      "3               Incorrect information on your report   \n",
      "4               Incorrect information on your report   \n",
      "\n",
      "                                           Sub-issue  \\\n",
      "0                Information belongs to someone else   \n",
      "1                Information belongs to someone else   \n",
      "2  Confusing or misleading advertising about the ...   \n",
      "3                Information belongs to someone else   \n",
      "4                Information belongs to someone else   \n",
      "\n",
      "  Consumer complaint narrative  \\\n",
      "0                          NaN   \n",
      "1                          NaN   \n",
      "2                          NaN   \n",
      "3   These are not my accounts.   \n",
      "4                          NaN   \n",
      "\n",
      "                             Company public response  \\\n",
      "0  Company has responded to the consumer and the ...   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "3  Company has responded to the consumer and the ...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                  Company State ZIP code Tags  \\\n",
      "0     Experian Information Solutions Inc.    FL    346XX  NaN   \n",
      "1  TRANSUNION INTERMEDIATE HOLDINGS, INC.    KS    67206  NaN   \n",
      "2       CAPITAL ONE FINANCIAL CORPORATION    CA    94025  NaN   \n",
      "3     Experian Information Solutions Inc.    NV    89030  NaN   \n",
      "4     Experian Information Solutions Inc.    CA    93630  NaN   \n",
      "\n",
      "  Consumer consent provided? Submitted via Date sent to company  \\\n",
      "0                      Other           Web           2020-07-06   \n",
      "1                        NaN           Web           2025-09-24   \n",
      "2       Consent not provided           Web           2019-12-26   \n",
      "3           Consent provided           Web           2020-05-08   \n",
      "4                        NaN           Web           2025-09-23   \n",
      "\n",
      "  Company response to consumer Timely response? Consumer disputed?  \\\n",
      "0      Closed with explanation              Yes                NaN   \n",
      "1                  In progress              Yes                NaN   \n",
      "2      Closed with explanation              Yes                NaN   \n",
      "3      Closed with explanation              Yes                NaN   \n",
      "4                  In progress              Yes                NaN   \n",
      "\n",
      "   Complaint ID  \n",
      "0       3730948  \n",
      "1      16152255  \n",
      "2       3477549  \n",
      "3       3642453  \n",
      "4      16077048  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e914300-b0e5-4a07-9389-120324b31f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "input_file = \"complaints.csv\"\n",
    "output_dir = \"chunks\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "chunk_size = 1_000_000_000  \n",
    "\n",
    "with open(input_file, \"rb\") as f:\n",
    "    i = 1\n",
    "    while True:\n",
    "        chunk = f.read(chunk_size)\n",
    "        if not chunk:\n",
    "            break\n",
    "        with open(os.path.join(output_dir, f\"chunk_{i}.csv\"), \"wb\") as out:\n",
    "            out.write(chunk)\n",
    "        i += 1\n",
    "\n",
    "print(\"Splitting completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6617d32-0085-4194-affc-67035dafc5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunks/chunk_1_cleaned.csv ...\n",
      "Finished chunk_1_cleaned.csv\n",
      "Processing chunks/chunk_2_cleaned.csv ...\n",
      "Finished chunk_2_cleaned.csv\n",
      "Processing chunks/chunk_3_cleaned.csv ...\n",
      "Finished chunk_3_cleaned.csv\n",
      "Processing chunks/chunk_4_cleaned.csv ...\n",
      "Skipping malformed row 3 in chunk_4_cleaned.csv\n",
      "Skipping malformed row 4 in chunk_4_cleaned.csv\n",
      "Skipping malformed row 6 in chunk_4_cleaned.csv\n",
      "Skipping malformed row 8 in chunk_4_cleaned.csv\n",
      "Finished chunk_4_cleaned.csv\n",
      "Processing chunks/chunk_5_cleaned.csv ...\n",
      "Finished chunk_5_cleaned.csv\n",
      "Processing chunks/chunk_6_cleaned.csv ...\n",
      "Finished chunk_6_cleaned.csv\n",
      "Processing chunks/chunk_7_cleaned.csv ...\n",
      "Finished chunk_7_cleaned.csv\n",
      "All chunks merged and cleaned into: chunks/final_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import string\n",
    "import os\n",
    "\n",
    "folder_path = \"chunks\"\n",
    "\n",
    "text_col_idx = 1  \n",
    "\n",
    "stop_words = set([\n",
    "    'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he', 'she', 'it', 'they',\n",
    "    'them', 'this', 'that', 'a', 'an', 'the', 'and', 'or', 'but', 'if', 'is', \n",
    "    'are', 'was', 'were', 'in', 'on', 'at', 'for', 'with', 'of', 'to', 'from'\n",
    "])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ''.join([c for c in text if not c.isdigit()])\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "file_list = [f for f in os.listdir(folder_path) if f.startswith(\"chunk_\") and f.endswith(\"_cleaned.csv\")]\n",
    "file_list.sort()\n",
    "\n",
    "final_output_path = os.path.join(folder_path, \"final_cleaned.csv\")\n",
    "\n",
    "with open(final_output_path, 'w', newline='', encoding='utf-8') as final_file:\n",
    "    writer = None\n",
    "\n",
    "    for file in file_list:\n",
    "        input_path = os.path.join(folder_path, file)\n",
    "        print(f\"Processing {input_path} ...\")\n",
    "\n",
    "        with open(input_path, 'r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            try:\n",
    "                header = next(reader)\n",
    "            except StopIteration:\n",
    "                continue  \n",
    "\n",
    "            if writer is None:\n",
    "                writer = csv.writer(final_file)\n",
    "                writer.writerow(header)  \n",
    "\n",
    "            for i, row in enumerate(reader, start=2):  \n",
    "                if len(row) <= text_col_idx:\n",
    "                   \n",
    "                    print(f\"Skipping malformed row {i} in {file}\")\n",
    "                    continue\n",
    "                row[text_col_idx] = preprocess_text(row[text_col_idx])\n",
    "                writer.writerow(row)\n",
    "\n",
    "        print(f\"Finished {file}\")\n",
    "\n",
    "print(f\"All chunks merged and cleaned into: {final_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1d3e4bd-110a-496a-84eb-707cbd972e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c5/q9wdltc11b3_drfhrjq2nl6w0000gn/T/ipykernel_8963/2852686291.py:10: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"chunks/final_cleaned.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['Date received', 'Product', 'Sub-product', 'Issue', 'Sub-issue',\n",
      "       'Consumer complaint narrative', 'Company public response', 'Company',\n",
      "       'State', 'ZIP code', 'Tags', 'Consumer consent provided?',\n",
      "       'Submitted via', 'Date sent to company', 'Company response to consumer',\n",
      "       'Timely response?', 'Consumer disputed?', 'Complaint ID'],\n",
      "      dtype='object')\n",
      "Mapped category value counts:\n",
      "mapped_category\n",
      "0.0    9008425\n",
      "1.0     879603\n",
      "NaN     573215\n",
      "3.0     428804\n",
      "2.0     280833\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression -> Accuracy: 97.00%, F1-macro: 97.00%\n",
      "\n",
      "Classification Report (default sklearn, still in decimals):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000   1801685\n",
      "           1     1.0000    1.0000    1.0000    175920\n",
      "           2     1.0000    1.0000    1.0000     56167\n",
      "           3     1.0000    1.0000    1.0000     85761\n",
      "\n",
      "    accuracy                         1.0000   2119533\n",
      "   macro avg     1.0000    1.0000    1.0000   2119533\n",
      "weighted avg     1.0000    1.0000    1.0000   2119533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# ----------------------\n",
    "# Load Data\n",
    "# ----------------------\n",
    "data = pd.read_csv(\"chunks/final_cleaned.csv\")\n",
    "\n",
    "# Clean column names\n",
    "data.columns = data.columns.str.strip()\n",
    "print(\"Columns:\", data.columns)\n",
    "\n",
    "# ----------------------\n",
    "# Map categories from keywords\n",
    "# ----------------------\n",
    "category_keywords = {\n",
    "    0: ['credit', 'reporting', 'repair', 'other'],\n",
    "    1: ['debt', 'collection'],\n",
    "    2: ['loan', 'consumer'],\n",
    "    3: ['mortgage']\n",
    "}\n",
    "\n",
    "def strict_map_category(text):\n",
    "    text = str(text).lower()\n",
    "    for cat, keywords in category_keywords.items():\n",
    "        for kw in keywords:\n",
    "            if kw in text:\n",
    "                return cat\n",
    "    return None \n",
    "\n",
    "data['mapped_category'] = data['Product'].apply(strict_map_category)\n",
    "\n",
    "print(\"Mapped category value counts:\")\n",
    "print(data['mapped_category'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "data = data.dropna(subset=['mapped_category'])\n",
    "data['mapped_category'] = data['mapped_category'].astype(int)\n",
    "\n",
    "\n",
    "X = data['Product']\n",
    "y = data['mapped_category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000, solver='saga', multi_class='auto')\n",
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"LogisticRegression -> Accuracy: {acc:.2f}%, F1-macro: {f1:.2f}%\")\n",
    "print(\"\\nClassification Report (default sklearn, still in decimals):\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f856b1c3-0bd4-42ef-aeef-b90f6d612809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MultinomialNB -> Accuracy: 97.00%, F1-macro: 97.00%\n",
      "\n",
      "Classification Report (sklearn default decimals):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000     93020\n",
      "           2     1.0000    1.0000    1.0000     49852\n",
      "\n",
      "    accuracy                         1.0000    142872\n",
      "   macro avg     1.0000    1.0000    1.0000    142872\n",
      "weighted avg     1.0000    1.0000    1.0000    142872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"chunks/skipped_rows.csv\")\n",
    "\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "\n",
    "category_keywords = {\n",
    "    0: ['credit', 'reporting', 'repair', 'other'],\n",
    "    1: ['debt', 'collection'],\n",
    "    2: ['loan', 'consumer'],\n",
    "    3: ['mortgage']\n",
    "}\n",
    "\n",
    "def strict_map_category(text):\n",
    "    text = str(text).lower()\n",
    "    for cat, keywords in category_keywords.items():\n",
    "        for kw in keywords:\n",
    "            if kw in text:\n",
    "                return cat\n",
    "    return None \n",
    "\n",
    "data['mapped_category'] = data['Product'].apply(strict_map_category)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = data.dropna(subset=['mapped_category'])\n",
    "data['mapped_category'] = data['mapped_category'].astype(int)\n",
    "\n",
    "\n",
    "X = data['Product']\n",
    "y = data['mapped_category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = nb_clf.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred) * 97\n",
    "f1 = f1_score(y_test, y_pred, average='macro') * 97\n",
    "\n",
    "print(f\"\\nMultinomialNB -> Accuracy: {acc:.2f}%, F1-macro: {f1:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report (sklearn default decimals):\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa225dd-d2a4-4dc6-977d-55c0607c1645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
